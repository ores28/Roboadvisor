{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tmax    \n",
      "0  \t500   \t0.57979\n",
      "1  \t348   \t0.695658\n",
      "2  \t320   \t0.709883\n",
      "3  \t320   \t0.716485\n",
      "4  \t328   \t0.716485\n",
      "5  \t331   \t0.795752\n",
      "6  \t306   \t0.818137\n",
      "7  \t332   \t6.16621 \n",
      "8  \t331   \t6.16621 \n",
      "9  \t326   \t6.16621 \n",
      "10 \t317   \t6.16621 \n",
      "11 \t356   \t6.16621 \n",
      "12 \t339   \tnan     \n",
      "13 \t307   \tnan     \n",
      "14 \t323   \tnan     \n",
      "15 \t315   \tnan     \n",
      "16 \t342   \tnan     \n",
      "17 \t339   \tnan     \n",
      "18 \t327   \tnan     \n",
      "19 \t344   \tnan     \n",
      "20 \t309   \tnan     \n",
      "21 \t308   \tnan     \n",
      "22 \t357   \t6.16621 \n",
      "23 \t338   \t6.16621 \n",
      "24 \t330   \tnan     \n",
      "25 \t342   \tnan     \n",
      "26 \t298   \tnan     \n",
      "27 \t328   \tnan     \n",
      "28 \t324   \t6.16621 \n",
      "29 \t333   \tnan     \n",
      "30 \t313   \tnan     \n",
      "31 \t336   \tnan     \n",
      "32 \t316   \tnan     \n",
      "33 \t320   \tnan     \n",
      "34 \t343   \t6.16621 \n",
      "35 \t319   \t6.16621 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Load all CSV files and add source column\n",
    "file_paths = [\n",
    "    (\"cmf2_final_results_with_ip_value.csv\", \"cmf2\"),\n",
    "    (\"nb2_final_results_with_ip_value.csv\", \"nb2\"),\n",
    "    (\"nb3_final_results_with_ip_value.csv\", \"nb3\"),\n",
    "    (\"gibf1_final_results_with_ip_value.csv\", \"gibf1\")\n",
    "]\n",
    "\n",
    "# Read data and combine into one dataframe\n",
    "dfs = []\n",
    "for file_path, source in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"Source\"] = source  # Add source information\n",
    "    df[\"Asset_Class\"] = f\"Asset_{file_paths.index((file_path, source)) + 1}\"  # Asset_1, Asset_2, ...\n",
    "    dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Extract relevant columns\n",
    "data = data[[\"Source\", \"Asset_Class\", \"Index\", \"Actual_ClosePrice\", \"Predicted_ClosePrice\", \"ip_value\"]]\n",
    "\n",
    "# Compute returns\n",
    "data[\"Return\"] = (data[\"Predicted_ClosePrice\"] - data[\"Actual_ClosePrice\"]) / data[\"Actual_ClosePrice\"]\n",
    "\n",
    "# Genetic Algorithm Parameters\n",
    "POP_SIZE = 500  # Increased population size\n",
    "CROSSOVER_RATE = 0.6  # Slightly decreased crossover rate\n",
    "MUTATION_RATE = 0.15  # Slightly increased mutation rate\n",
    "GENERATIONS = 2000  # Increased number of generations\n",
    "ELITE_SIZE = 10  # Increased elite size\n",
    "\n",
    "# Fitness function: Sharpe Ratio\n",
    "def sharpe_ratio(thresholds):\n",
    "    selected_assets = []\n",
    "    \n",
    "    # thresholds is a list with one threshold per asset class\n",
    "    for i, asset_class in enumerate(data[\"Asset_Class\"].unique()):\n",
    "        asset_data = data[data[\"Asset_Class\"] == asset_class]\n",
    "        selected_data = asset_data[asset_data[\"ip_value\"] >= thresholds[i]]\n",
    "        \n",
    "        if not selected_data.empty:\n",
    "            selected_assets.append(selected_data)\n",
    "    \n",
    "    if len(selected_assets) == 0:\n",
    "        return (-1.0,)  # Penalty for no assets selected\n",
    "    \n",
    "    selected_assets = pd.concat(selected_assets)\n",
    "    \n",
    "    # Portfolio Returns\n",
    "    portfolio_return = selected_assets[\"Return\"].mean()\n",
    "    portfolio_std = selected_assets[\"Return\"].std()\n",
    "\n",
    "    if portfolio_std <= 1e-5:  # Avoid division by zero\n",
    "        return (-1.0,)  # Penalty for zero standard deviation\n",
    "    \n",
    "    sharpe = portfolio_return / portfolio_std\n",
    "    return (sharpe,)  # Return a tuple\n",
    "\n",
    "# Genetic Algorithm Setup\n",
    "if not hasattr(creator, \"FitnessMax\"):\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "if not hasattr(creator, \"Individual\"):\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", np.random.uniform, data[\"ip_value\"].min(), data[\"ip_value\"].max())\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=len(data[\"Asset_Class\"].unique()))  # One threshold per asset class\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.2, indpb=MUTATION_RATE)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", sharpe_ratio)  # Directly use sharpe_ratio\n",
    "\n",
    "# Run Genetic Algorithm\n",
    "def run_ga():\n",
    "    population = toolbox.population(n=POP_SIZE)\n",
    "    hof = tools.HallOfFame(ELITE_SIZE)  # Keep best individuals\n",
    "\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    algorithms.eaSimple(population, toolbox, cxpb=CROSSOVER_RATE, mutpb=MUTATION_RATE,\n",
    "                        ngen=GENERATIONS, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    return hof[0]\n",
    "\n",
    "# Get the optimized ip-value thresholds\n",
    "best_thresholds = run_ga()\n",
    "\n",
    "# Create a DataFrame to track the asset class, source, and optimized thresholds\n",
    "thresholds_df = pd.DataFrame({\n",
    "    \"Asset_Class\": data[\"Asset_Class\"].unique(),\n",
    "    \"Source\": [file_path[1] for file_path in file_paths],\n",
    "    \"Optimized_Threshold\": best_thresholds\n",
    "})\n",
    "\n",
    "# Print the DataFrame to see the optimized thresholds for each asset\n",
    "print(thresholds_df)\n",
    "\n",
    "# Select assets based on optimized thresholds\n",
    "selected_assets = []\n",
    "for i, asset_class in enumerate(data[\"Asset_Class\"].unique()):\n",
    "    asset_data = data[data[\"Asset_Class\"] == asset_class]\n",
    "    selected_data = asset_data[asset_data[\"ip_value\"] >= best_thresholds[i]]\n",
    "    if not selected_data.empty:\n",
    "        selected_assets.append(selected_data)\n",
    "\n",
    "final_portfolio = pd.concat(selected_assets)\n",
    "\n",
    "# Implement Risk Parity Model for Weights\n",
    "if not final_portfolio.empty:\n",
    "    volatilities = final_portfolio.groupby(\"Asset_Class\")[\"Return\"].std()\n",
    "    volatilities = volatilities.replace(0, 1e-5)  # Avoid division by zero\n",
    "    risk_parity_weights = 1 / volatilities  # Inverse volatility\n",
    "    risk_parity_weights /= risk_parity_weights.sum()  # Normalize to sum to 1\n",
    "\n",
    "    # Assign weights\n",
    "    final_portfolio[\"Weight\"] = final_portfolio[\"Asset_Class\"].map(risk_parity_weights)\n",
    "else:\n",
    "    final_portfolio[\"Weight\"] = 0  # If no assets are selected, set weight to 0\n",
    "\n",
    "# Save final portfolio\n",
    "final_portfolio.to_csv(\"optimized_portfolio_1.csv\", index=False)\n",
    "print(\"Optimized portfolio saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Asset_Class Source  Optimized_Threshold\n",
      "0     Asset_1   cmf2         8.906469e-01\n",
      "1     Asset_2    nb2         1.070586e+18\n",
      "2     Asset_3    nb3         1.058429e+19\n",
      "3     Asset_4  gibf1         3.169559e+16\n"
     ]
    }
   ],
   "source": [
    "print(thresholds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_df.to_csv(\"thresholds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8906469413364329, 1.0705860018214318e+18, 1.058428980706035e+19, 3.1695592453533588e+16]\n"
     ]
    }
   ],
   "source": [
    "print(best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
